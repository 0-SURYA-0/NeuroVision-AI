{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb847186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… System path configured for 3D architecture modules\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(r\"c:\\Users\\surya\\NueroVisionAI\\core\\3D architecture\")\n",
    "\n",
    "print(\"âœ… System path configured for 3D architecture modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24441ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
      "3D BRAIN TUMOR SEGMENTATION EVALUATION\n",
      "COMPREHENSIVE RESULTS ANALYSIS\n",
      "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from metrics_3d import SegmentationMetrics\n",
    "from utils_3d import Config, format_time\n",
    "from visualize_3d import VolumeVisualizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ§ \" * 50)\n",
    "print(\"3D BRAIN TUMOR SEGMENTATION EVALUATION\")\n",
    "print(\"COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "print(\"ğŸ§ \" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017b9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ CHECKING MODEL ARTIFACTS:\n",
      "â”œâ”€â”€ Artifacts directory: c:\\Users\\surya\\NueroVisionAI\\notebooks\\3D\\artifacts\n",
      "â”œâ”€â”€ Outputs directory: c:\\Users\\surya\\NueroVisionAI\\outputs\\3d\n",
      "â””â”€â”€ Checkpoints directory: c:\\Users\\surya\\NueroVisionAI\\checkpoints\\3d\n",
      "\n",
      "âœ… TRAINED MODELS FOUND:\n",
      "â”œâ”€â”€ best_model.pth (PyTorch checkpoint)\n",
      "â”œâ”€â”€ artifacts/best_model.pth\n",
      "â”œâ”€â”€ tumornet_best.pkl\n",
      "\n",
      "ğŸ“ˆ LOADING TRAINING HISTORY:\n",
      "â”œâ”€â”€ History file: training_history.json\n",
      "â”œâ”€â”€ Total epochs: 7\n",
      "â””â”€â”€ Data loaded successfully\n",
      "\n",
      "ğŸ” SCANNING FOR INFERENCE RESULTS:\n",
      "â”œâ”€â”€ No inference results directory found\n",
      "\n",
      "ğŸ“Š INFERENCE SUMMARY LOADED:\n",
      "â”œâ”€â”€ Total cases: 209\n",
      "â”œâ”€â”€ Successful cases: 0\n",
      "â””â”€â”€ Processing time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.data_dir = r\"c:\\Users\\surya\\NueroVisionAI\\data\\BrainWithTumor\"\n",
    "config.output_dir = r\"c:\\Users\\surya\\NueroVisionAI\\outputs\\3d\"\n",
    "config.checkpoint_dir = r\"c:\\Users\\surya\\NueroVisionAI\\checkpoints\\3d\"\n",
    "\n",
    "artifacts_dir = Path(r\"c:\\Users\\surya\\NueroVisionAI\\notebooks\\3D\\artifacts\")\n",
    "outputs_dir = Path(config.output_dir)\n",
    "checkpoints_dir = Path(config.checkpoint_dir)\n",
    "\n",
    "print(f\"\\nğŸ“‚ CHECKING MODEL ARTIFACTS:\")\n",
    "print(f\"â”œâ”€â”€ Artifacts directory: {artifacts_dir}\")\n",
    "print(f\"â”œâ”€â”€ Outputs directory: {outputs_dir}\")\n",
    "print(f\"â””â”€â”€ Checkpoints directory: {checkpoints_dir}\")\n",
    "\n",
    "model_files_found = []\n",
    "if (checkpoints_dir / \"best_model.pth\").exists():\n",
    "    model_files_found.append(\"best_model.pth (PyTorch checkpoint)\")\n",
    "if (artifacts_dir / \"best_model.pth\").exists():\n",
    "    model_files_found.append(\"artifacts/best_model.pth\")\n",
    "if (artifacts_dir / \"tumornet_best.pkl\").exists():\n",
    "    model_files_found.append(\"tumornet_best.pkl\")\n",
    "\n",
    "if model_files_found:\n",
    "    print(f\"\\nâœ… TRAINED MODELS FOUND:\")\n",
    "    for model_file in model_files_found:\n",
    "        print(f\"â”œâ”€â”€ {model_file}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ No trained models found. Run training notebook first!\")\n",
    "\n",
    "history_files = list(artifacts_dir.glob(\"*history*.json\"))\n",
    "if history_files:\n",
    "    print(f\"\\nğŸ“ˆ LOADING TRAINING HISTORY:\")\n",
    "    try:\n",
    "        with open(history_files[0], 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        print(f\"â”œâ”€â”€ History file: {history_files[0].name}\")\n",
    "        \n",
    "        if isinstance(training_history, dict):\n",
    "            epoch_count = len(training_history.get('train_loss', training_history.get('loss', [])))\n",
    "            print(f\"â”œâ”€â”€ Total epochs: {epoch_count}\")\n",
    "        elif isinstance(training_history, list):\n",
    "            print(f\"â”œâ”€â”€ Total epochs: {len(training_history)}\")\n",
    "        else:\n",
    "            print(f\"â”œâ”€â”€ History format: {type(training_history)}\")\n",
    "        \n",
    "        print(f\"â””â”€â”€ Data loaded successfully\")\n",
    "    except Exception as e:\n",
    "        training_history = None\n",
    "        print(f\"â”œâ”€â”€ Error loading history: {e}\")\n",
    "        print(f\"â””â”€â”€ Continuing without training history\")\n",
    "else:\n",
    "    training_history = None\n",
    "    print(f\"\\nâš ï¸  No training history found\")\n",
    "\n",
    "print(f\"\\nğŸ” SCANNING FOR INFERENCE RESULTS:\")\n",
    "inference_results_dir = outputs_dir / \"inference_results\"\n",
    "if inference_results_dir.exists():\n",
    "    segmentation_files = list(inference_results_dir.glob(\"*_segmentation.nii.gz\"))\n",
    "    probability_files = list(inference_results_dir.glob(\"*_probability.nii.gz\"))\n",
    "    uncertainty_files = list(inference_results_dir.glob(\"*_uncertainty.nii.gz\"))\n",
    "    \n",
    "    print(f\"â”œâ”€â”€ Segmentation masks: {len(segmentation_files)}\")\n",
    "    print(f\"â”œâ”€â”€ Probability maps: {len(probability_files)}\")\n",
    "    print(f\"â””â”€â”€ Uncertainty maps: {len(uncertainty_files)}\")\n",
    "else:\n",
    "    segmentation_files = []\n",
    "    probability_files = []\n",
    "    uncertainty_files = []\n",
    "    print(f\"â”œâ”€â”€ No inference results directory found\")\n",
    "\n",
    "summary_report_path = outputs_dir / \"inference_summary_report.json\"\n",
    "if summary_report_path.exists():\n",
    "    try:\n",
    "        with open(summary_report_path, 'r') as f:\n",
    "            inference_summary = json.load(f)\n",
    "        print(f\"\\nğŸ“Š INFERENCE SUMMARY LOADED:\")\n",
    "        print(f\"â”œâ”€â”€ Total cases: {inference_summary['inference_summary']['total_cases']}\")\n",
    "        print(f\"â”œâ”€â”€ Successful cases: {inference_summary['inference_summary']['successful_cases']}\")\n",
    "        print(f\"â””â”€â”€ Processing time: {inference_summary['inference_summary']['total_processing_time_seconds']:.2f}s\")\n",
    "    except Exception as e:\n",
    "        inference_summary = None\n",
    "        print(f\"\\nâš ï¸  Error loading inference summary: {e}\")\n",
    "else:\n",
    "    inference_summary = None\n",
    "    print(f\"\\nâš ï¸  No inference summary report found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd5cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš ï¸  No segmentation files found for analysis\n",
      "â””â”€â”€ Run inference notebook first to generate results\n"
     ]
    }
   ],
   "source": [
    "if segmentation_files:\n",
    "    print(f\"\\nğŸ§  ANALYZING SEGMENTATION RESULTS...\")\n",
    "    \n",
    "    metrics_calculator = SegmentationMetrics()\n",
    "    case_analyses = []\n",
    "    \n",
    "    for i, seg_file in enumerate(segmentation_files[:10]):\n",
    "        case_id = seg_file.stem.replace('_segmentation', '')\n",
    "        print(f\"\\nâ”œâ”€â”€ Analyzing Case: {case_id}\")\n",
    "        \n",
    "        try:\n",
    "            seg_img = nib.load(seg_file)\n",
    "            segmentation = seg_img.get_fdata()\n",
    "            \n",
    "            prob_file = inference_results_dir / f\"{case_id}_probability.nii.gz\"\n",
    "            if prob_file.exists():\n",
    "                prob_img = nib.load(prob_file)\n",
    "                probability_map = prob_img.get_fdata()\n",
    "            else:\n",
    "                probability_map = None\n",
    "            \n",
    "            uncertainty_file = inference_results_dir / f\"{case_id}_uncertainty.nii.gz\"\n",
    "            if uncertainty_file.exists():\n",
    "                unc_img = nib.load(uncertainty_file)\n",
    "                uncertainty_map = unc_img.get_fdata()\n",
    "            else:\n",
    "                uncertainty_map = None\n",
    "            \n",
    "            tumor_volume = np.sum(segmentation > 0)\n",
    "            total_volume = segmentation.size\n",
    "            tumor_percentage = (tumor_volume / total_volume) * 100\n",
    "            \n",
    "            max_probability = np.max(probability_map) if probability_map is not None else 0\n",
    "            mean_tumor_prob = np.mean(probability_map[segmentation > 0]) if probability_map is not None and tumor_volume > 0 else 0\n",
    "            mean_uncertainty = np.mean(uncertainty_map[segmentation > 0]) if uncertainty_map is not None and tumor_volume > 0 else 0\n",
    "            \n",
    "            analysis = {\n",
    "                'case_id': case_id,\n",
    "                'tumor_volume_voxels': int(tumor_volume),\n",
    "                'tumor_percentage': round(tumor_percentage, 2),\n",
    "                'max_probability': round(max_probability, 4),\n",
    "                'mean_tumor_probability': round(mean_tumor_prob, 4),\n",
    "                'mean_uncertainty': round(mean_uncertainty, 4),\n",
    "                'volume_shape': segmentation.shape,\n",
    "                'has_tumor': tumor_volume > 0\n",
    "            }\n",
    "            \n",
    "            case_analyses.append(analysis)\n",
    "            \n",
    "            print(f\"    â”œâ”€â”€ Tumor volume: {tumor_volume:,} voxels ({tumor_percentage:.1f}%)\")\n",
    "            print(f\"    â”œâ”€â”€ Max probability: {max_probability:.4f}\")\n",
    "            print(f\"    â””â”€â”€ Mean tumor prob: {mean_tumor_prob:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    â””â”€â”€ âŒ Error analyzing {case_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nğŸ“Š SEGMENTATION STATISTICS:\")\n",
    "    if case_analyses:\n",
    "        tumor_cases = [c for c in case_analyses if c['has_tumor']]\n",
    "        no_tumor_cases = [c for c in case_analyses if not c['has_tumor']]\n",
    "        \n",
    "        print(f\"â”œâ”€â”€ Total cases analyzed: {len(case_analyses)}\")\n",
    "        print(f\"â”œâ”€â”€ Cases with detected tumors: {len(tumor_cases)}\")\n",
    "        print(f\"â”œâ”€â”€ Cases with no tumors: {len(no_tumor_cases)}\")\n",
    "        print(f\"â””â”€â”€ Detection rate: {len(tumor_cases)/len(case_analyses)*100:.1f}%\")\n",
    "        \n",
    "        if tumor_cases:\n",
    "            volumes = [c['tumor_volume_voxels'] for c in tumor_cases]\n",
    "            percentages = [c['tumor_percentage'] for c in tumor_cases]\n",
    "            probabilities = [c['max_probability'] for c in tumor_cases]\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ TUMOR CHARACTERISTICS:\")\n",
    "            print(f\"â”œâ”€â”€ Average volume: {np.mean(volumes):.0f} Â± {np.std(volumes):.0f} voxels\")\n",
    "            print(f\"â”œâ”€â”€ Volume range: {np.min(volumes):,} - {np.max(volumes):,} voxels\")\n",
    "            print(f\"â”œâ”€â”€ Average percentage: {np.mean(percentages):.2f} Â± {np.std(percentages):.2f}%\")\n",
    "            print(f\"â”œâ”€â”€ Average max prob: {np.mean(probabilities):.4f} Â± {np.std(probabilities):.4f}\")\n",
    "            print(f\"â””â”€â”€ Probability range: {np.min(probabilities):.4f} - {np.max(probabilities):.4f}\")\n",
    "\n",
    "else:\n",
    "    case_analyses = []\n",
    "    print(f\"\\nâš ï¸  No segmentation files found for analysis\")\n",
    "    print(f\"â””â”€â”€ Run inference notebook first to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c32846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ COMPREHENSIVE EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ TRAINING PERFORMANCE:\n",
      "â”œâ”€â”€ History entries: 7\n",
      "â”œâ”€â”€ Latest loss: N/A\n",
      "â””â”€â”€ Latest dice: N/A\n",
      "\n",
      "ğŸ† MODEL CHECKPOINT INFO:\n",
      "â”œâ”€â”€ Best val Dice: 0.8974\n",
      "â”œâ”€â”€ Epoch: 17\n",
      "â”œâ”€â”€ Learning rate: N/A\n",
      "â””â”€â”€ Checkpoint saved\n",
      "\n",
      "ğŸ’¾ DATA PROCESSING SUMMARY:\n",
      "â”œâ”€â”€ BrainWithTumor: 1620 files\n",
      "â”œâ”€â”€ Healthy: 1620 files\n",
      "â”œâ”€â”€ TumorOnly: 1620 files\n",
      "â””â”€â”€ Total data files: 4860\n",
      "\n",
      "ğŸš€ INFERENCE PERFORMANCE:\n",
      "â”œâ”€â”€ Total test cases: 209\n",
      "â”œâ”€â”€ Successful inferences: 0\n",
      "â”œâ”€â”€ Failed cases: 209\n",
      "â”œâ”€â”€ Success rate: 0.0%\n",
      "â”œâ”€â”€ Total processing time: 0.0s\n",
      "â”œâ”€â”€ Average time per case: 0.00s\n",
      "\n",
      "ğŸ“ OUTPUT DIRECTORIES:\n",
      "â”œâ”€â”€ Model artifacts: c:\\Users\\surya\\NueroVisionAI\\notebooks\\3D\\artifacts\n",
      "â”œâ”€â”€ Inference results: c:\\Users\\surya\\NueroVisionAI\\outputs\\3d\\inference_results\n",
      "â”œâ”€â”€ Checkpoints: c:\\Users\\surya\\NueroVisionAI\\checkpoints\\3d\n",
      "â””â”€â”€ Summary reports: c:\\Users\\surya\\NueroVisionAI\\outputs\\3d\n",
      "\n",
      "ğŸ‰ EVALUATION COMPLETED SUCCESSFULLY!\n",
      "ğŸ”¥ COMPREHENSIVE ANALYSIS | DETAILED METRICS | PRODUCTION READY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ COMPREHENSIVE EVALUATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if training_history:\n",
    "    print(f\"\\nğŸ“ˆ TRAINING PERFORMANCE:\")\n",
    "    \n",
    "    if isinstance(training_history, dict):\n",
    "        train_losses = training_history.get('train_loss', training_history.get('loss', []))\n",
    "        val_losses = training_history.get('val_loss', training_history.get('val_loss', []))\n",
    "        val_dice_scores = training_history.get('val_dice', training_history.get('dice', []))\n",
    "        \n",
    "        if train_losses:\n",
    "            print(f\"â”œâ”€â”€ Total epochs: {len(train_losses)}\")\n",
    "            print(f\"â”œâ”€â”€ Final train loss: {train_losses[-1]:.4f}\")\n",
    "            if val_losses:\n",
    "                print(f\"â”œâ”€â”€ Final val loss: {val_losses[-1]:.4f}\")\n",
    "            if val_dice_scores:\n",
    "                best_dice = max(val_dice_scores)\n",
    "                best_epoch = val_dice_scores.index(best_dice) + 1\n",
    "                print(f\"â”œâ”€â”€ Best val Dice: {best_dice:.4f} (epoch {best_epoch})\")\n",
    "                print(f\"â””â”€â”€ Final val Dice: {val_dice_scores[-1]:.4f}\")\n",
    "    elif isinstance(training_history, list):\n",
    "        print(f\"â”œâ”€â”€ History entries: {len(training_history)}\")\n",
    "        if training_history and isinstance(training_history[0], dict):\n",
    "            latest_entry = training_history[-1]\n",
    "            print(f\"â”œâ”€â”€ Latest loss: {latest_entry.get('loss', 'N/A')}\")\n",
    "            print(f\"â””â”€â”€ Latest dice: {latest_entry.get('dice', 'N/A')}\")\n",
    "\n",
    "checkpoint_path = checkpoints_dir / \"best_model.pth\"\n",
    "if checkpoint_path.exists():\n",
    "    try:\n",
    "        import torch\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "        print(f\"\\nğŸ† MODEL CHECKPOINT INFO:\")\n",
    "        print(f\"â”œâ”€â”€ Best val Dice: {checkpoint.get('best_val_dice', 'N/A'):.4f}\")\n",
    "        print(f\"â”œâ”€â”€ Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "        print(f\"â”œâ”€â”€ Learning rate: {checkpoint.get('learning_rate', 'N/A')}\")\n",
    "        print(f\"â””â”€â”€ Checkpoint saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  Error loading checkpoint: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ DATA PROCESSING SUMMARY:\")\n",
    "data_folders = [\"BrainWithTumor\", \"Healthy\", \"TumorOnly\"]\n",
    "total_files = 0\n",
    "for folder in data_folders:\n",
    "    folder_path = Path(config.data_dir).parent / folder\n",
    "    if folder_path.exists():\n",
    "        count = len(list(folder_path.glob(\"*.nii.gz\")))\n",
    "        total_files += count\n",
    "        print(f\"â”œâ”€â”€ {folder}: {count} files\")\n",
    "    else:\n",
    "        print(f\"â”œâ”€â”€ {folder}: Directory not found\")\n",
    "\n",
    "print(f\"â””â”€â”€ Total data files: {total_files}\")\n",
    "\n",
    "if inference_summary:\n",
    "    inference_info = inference_summary['inference_summary']\n",
    "    print(f\"\\nğŸš€ INFERENCE PERFORMANCE:\")\n",
    "    print(f\"â”œâ”€â”€ Total test cases: {inference_info['total_cases']}\")\n",
    "    print(f\"â”œâ”€â”€ Successful inferences: {inference_info['successful_cases']}\")\n",
    "    print(f\"â”œâ”€â”€ Failed cases: {inference_info['failed_cases']}\")\n",
    "    \n",
    "    if inference_info['total_cases'] > 0:\n",
    "        success_rate = inference_info['successful_cases']/inference_info['total_cases']*100\n",
    "        print(f\"â”œâ”€â”€ Success rate: {success_rate:.1f}%\")\n",
    "    else:\n",
    "        print(f\"â”œâ”€â”€ Success rate: N/A\")\n",
    "    \n",
    "    print(f\"â”œâ”€â”€ Total processing time: {format_time(inference_info['total_processing_time_seconds'])}\")\n",
    "    print(f\"â”œâ”€â”€ Average time per case: {inference_info['average_time_per_case_seconds']:.2f}s\")\n",
    "    \n",
    "    case_results = inference_summary.get('case_results', [])\n",
    "    if case_results:\n",
    "        tumor_detected_cases = [c for c in case_results if c['tumor_detected']]\n",
    "        detection_rate = len(tumor_detected_cases)/len(case_results)*100\n",
    "        print(f\"â””â”€â”€ Tumor detection rate: {len(tumor_detected_cases)}/{len(case_results)} ({detection_rate:.1f}%)\")\n",
    "\n",
    "if 'case_analyses' in locals() and case_analyses:\n",
    "    print(f\"\\nğŸ”¬ DETAILED ANALYSIS RESULTS:\")\n",
    "    print(f\"â”œâ”€â”€ Cases analyzed: {len(case_analyses)}\")\n",
    "    \n",
    "    tumor_volumes = [c['tumor_volume_voxels'] for c in case_analyses if c['has_tumor']]\n",
    "    if tumor_volumes:\n",
    "        print(f\"â”œâ”€â”€ Avg tumor volume: {np.mean(tumor_volumes):.0f} voxels\")\n",
    "        print(f\"â”œâ”€â”€ Tumor volume std: {np.std(tumor_volumes):.0f} voxels\")\n",
    "        \n",
    "        tumor_probs = [c['mean_tumor_probability'] for c in case_analyses if c['has_tumor']]\n",
    "        print(f\"â”œâ”€â”€ Avg tumor confidence: {np.mean(tumor_probs):.4f}\")\n",
    "        print(f\"â””â”€â”€ Confidence std: {np.std(tumor_probs):.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ OUTPUT DIRECTORIES:\")\n",
    "print(f\"â”œâ”€â”€ Model artifacts: {artifacts_dir}\")\n",
    "if 'inference_results_dir' in locals():\n",
    "    print(f\"â”œâ”€â”€ Inference results: {inference_results_dir}\")\n",
    "print(f\"â”œâ”€â”€ Checkpoints: {checkpoints_dir}\")\n",
    "print(f\"â””â”€â”€ Summary reports: {outputs_dir}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"ğŸ”¥ COMPREHENSIVE ANALYSIS | DETAILED METRICS | PRODUCTION READY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
